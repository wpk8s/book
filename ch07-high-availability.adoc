[[ch07-high-availability]]
== High availability

For me, high availability simply means that if part of the infrastructure fails, the websites still run without problems for visitors, and the only problem for me is to fix the infrastructure.

Ideally, the infrastructure should self-heal and auto-scale, a more advanced topic to discussed later.

MicroK8s starts by default with HA enabled, and starts acting as a real highly available cluster the moment you add 2 or more extra nodes in the cluster, as the setup is 3 minimal nodes.

=== Local storage vs highly available storage

MicroK8s comes with two alternative storage solution addons, at one step distance to use any of it.

The one that existed since the beginning of the project, LOCAL, allowed containers to use volumes strictly if they run on the same host.

That has been a blocker to high availability, because if that specific node crashes, then data would not be readily available to continue running on other healthy nodes in the cluster.

=== Introducing OpenEBS

For High Availablity storage, OpenEBS on MicroK8s will be easy to use, and from the selected components we are going to setup, it will be light on hardware resources.

It does, however, come with a minimal resource requirement: a minimum of 3 nodes in the cluster. Well, that is also the specification of MicroK8s High Availablity cluster. The addon is pre-configured to replicate the volumes on a minimum of 3 nodes. This can be decreased to 2, or increased, but other values than 3 or 5 for highly available replicated services is strongly discouraged.

From this point, as we will be handling a High Availablity cluster, we will be using a setup with a minimum of 3 nodes all the time. In my proposed setup, I often go with 3 primary servers and 3 or more extra worker nodes. 

**What is OpenEBS offering us?**

Simply put, OpenEBS provides network compatible volumes with real-time mirroring of data, so in case of a node that was storing the data crashing, at least two others have the exact same data. If a container with a volume attached to it crashes due to a node failure, it is restarted very quickly on another node, and the volume is re-attached to it. Even if the node had a copy of the volumes data, failure does not affect it, as the volume can still run from the other sources. With a proper alert system in place, you could intervene and fix the node's issue in little to no time, or in my preference, I make sure I have the logs available, and simply create a new node in the cluster, and remove the old broken one. After getting my ideal state of the cluster back, I can now research on the problem without having the cluster in danger, and the visitors would not even know there was a problem.

Another advantage of OpenEBS is one volume could be shared by Pod replicas or even between Deployments. Meaning we will be able to use a volume attached to WordPress, spread over multiple nodes to increase performance during high traffic. We can add nodes, we can remove nodes. This helps keeping costs lower over time, without causing any downtime as you need in classic vertical scaling setups.

As OpenEBS comes with multiple components, we are able to select them on install, so next, we will only look into Jiva and NFS provisioner and exclude the others.

You should do your research about the extra components if you have edge cases. I selected Jiva versus Cstor or Mayastore, as Jiva is using least memory, least cpu and requires zero efforts when upgrading OpenEBS. Cstor might require extra steps on upgrades that involve extra knowledge, and Mayastore requires on each node extra hardware resources dedicated only for itself, which for the concept of MicroK8S might not fit most of users.

=== Setting up a Highly Available Cluster

Using the setup we had up until now, we can replicate the model from Chapter 4 at least 3 times to have a minimum of 3 nodes available to setup the cluster. Just stop right before `sudo microk8s enable dns ingress`.

The next step we need is to install and enable iscsi and nfs daemon, to allow OpenEBS to work.

On each node, run the following:

[source,bash]
----
sudo apt install open-iscsi
sudo systemctl enable --now iscsid
sudo apt install nfs-common
----

One small, but critical step before making extra nodes joining the cluster, is to set in hosts the hostnames of all nodes and their local network ip addresses. This step is needed, as MicroK8S will set the nodes to communicate using their hostnames and if skipped, when running the command on any node to join, it will exist with a timeout of not being able to communicate to the target node's hostname. Do this step even if you are using servers only with private network.

On each node, edit the `/etc/hosts` file and add all nodes you created with the hostnames set on creation. In my case, on Hetzner, I have to add in each hosts file the following:

[source,text]
----
172.16.0.2 microk8s-2
172.16.0.3 microk8s-3
172.16.0.4 microk8s-4
----

NOTE: In Hetzner, I'm usually starting the count from 2, as in the Web UI administration, when creating servers attached to a local network, dhcp assigns ip addresses starting from 2, in ascending order. If you need to have full manual control over ip addresses, you have to use directly the API, and it works even using curl or a graphical tool like Postman. Do not try to allocate first IP, as that is used as internet gateway for servers without a public IP address.

Now in one node only, run the command to get the add node command with token:

`sudo microk8s add-node`

The result should be similar to:

[source,text]
----
From the node you wish to join to this cluster, run the following:
microk8s join 135.181.91.189:25000/f6308cb5dea38cb7c785adfe347a214a/eaf81c74de94

If the node you are adding is not reachable through the default interface you can use one of the following:
 microk8s join 135.181.91.189:25000/f6308cb5dea38cb7c785adfe347a214a/eaf81c74de94
 microk8s join 172.16.0.3:25000/f6308cb5dea38cb7c785adfe347a214a/eaf81c74de94
----

Copy the line containing an ip from the local network attached and run it prefixed with `sudo` on your second node.

Repeat the process for each node, first get the command with token, then run it on the new node.

You can have as many nodes you want like this.

Next, on the first node, or another one if you would prefer, run:

`sudo microk8s enable dns ingress`

It will take up to a minute.

When you are done, if you run `sudo microk8s status` you should have a similar output to the following:

[source,text]
----
microk8s is running
high-availability: yes
  datastore master nodes: 172.16.0.3:19001 172.16.0.5:19001 172.16.0.2:19001
  datastore standby nodes: 172.16.0.7:19001 172.16.0.6:19001 172.16.0.4:19001
addons:
  enabled:
    dns                  # CoreDNS
    ha-cluster           # Configure high availability on the current node
    helm3                # Helm 3 - Kubernetes package manager
    ingress              # Ingress controller for external access
----

The rest of the information is about disabled addons, so you can ignore it.

The above, shows I have a cluster of 6 nodes, with 3 masters and 3 worker nodes that in case of a dead master, any could replace it. With more than 3 nodes I surely sleep well when 1-2 nodes crash during the night, as long as the total workload can be sustained by the healthy nodes. MicroK8S can add also work nodes only, which will not be promoted to primary in case of a primary node crash.

Regarding this, I usually calculate that I use 70-80% of available resources, but I don't go under 3 nodes, if my low traffic load can use 1 node only. We will talk about how to estimate resources later in the book.

Before we dive into WordPress, you need to redo the `cert-manager` addition, so the cluster will be capable of handling HTTPS needed certificates automatically. I will use the Letsencrypt option, as I prefer it. If you use alternatives, adjust the recipes to use the correct certificate.

=== Installing OpenEBS

I will use HELM to install Openebs and to be able to upgrade it easy, hopefully only by updating the repo and running the upgrade command.

Setup helm if you have not done it yet as described previously and run the following:

[source,text]
----
heml --namespace openebs --create-namespace install openebs openebs/openebs \
    --version 3.3.x \
    --set ndm.enabled=false \
    --set ndmOperator.enabled=false \
    --set jiva.enabled=true \
    --set jiva.storageClass.name=openebs-jiva \
    --set jiva.storageClass.isDefaultClass=true \
    --set jiva.csiNode.kubeletDir="/var/snap/microk8s/common/var/lib/kubelet/" \
    --set localprovisioner.basePath="/var/snap/microk8s/common/var/openebs/local" \
    --set varDirectoryPath.baseDir="/var/snap/microk8s/common/var/openebs" \
    --set nfs-provisioner.enabled=true \
    --set nfs-provisioner.nfsStorageClass.backendStorageClass=openebs-jiva
----

The above command will install OpenEBS with Jiva and NFS provisioner enabled, ensure any dependent components are installed and configured as well. On future upgrades, dependencies should be handled by helm as well.

This will provide us more storage classes, including a local volume one, useful for StatefulSets (example for MySQL with Innodb cluster for better performance). Our important storage classes are openebs-jiva and openebs-kernel-nfs, which the helm command preconfigured it to use openebs-jiva as backend.

Simple to understand how it works: under the hood, the real volumes are provisioned by the openebs-hostpath. Jiva will ask it to create on 3 available servers 3 volumes. Using iscsid, Jiva will be able to talk to each of them. The data will be "proxied" by Jiva, which will ensure that data is syncronous replicated on all nodes and read is done as fast as possible from one. This will provide a normal read-write-once as we know volume. On top of it, the openebs-kernel-nfs can expose the storage block as read-write-many, sharable within the same namespace. We can use each class with benefit of abstracting all that complexity from us, depending on each case we are solving, like highly available database, horizontal scalled wordpress etc. 

Let's now tweak our original WordPress recipe. When we see it working, we will manually experiment with node issues, simulating the common problems you might face when the public cloud provider has issues or certain nodes become unavailable for example (we would simply destroy the node, which would be equivalent for our cluster when node is not available because of the provider).

IMPORTANT: Database nodes can't be replicated by bumping up the number of replicas. If you are looking into Mysql/MariaDB replication, then the only easy way is https://mariadb.com/kb/en/galera-cluster/[Galera] as a separate cluster, or the recent promoted as stable MySQL Innodb Operator for Kubernetes. Choice depends a lot on your goals. I do a big Galera setup for one organization's database cluster shared for many websites and applications that security decisions allow this setup, or go for the MySQL Innodb Operator for each application requiring an isolated database cluster.

I will edit the **WordPress** recipe we used before now, replacing the local storage part with OpenEBS classes.

NOTE: To ensure that my recipes are kept safe, I use git to track changes. This way, I can run them from any node, anytime, and keep changes synced. Personally I use my first node like a "master" and do all operations from it alone, and if it dies, I pick the next one to be my "master", but as the recipes are kept in git, I can just pull the latest and everything is still there. One note on it, I keep secrets separate and add them by environment - I will show you later how I keep secrets safe even in git. These recipes could be even be shared openly, as they do not expose anything sensitive about your content and data.

Secrets stay like before:

.https://j.mp/3q0UdLp[kustomization.yml]
[source,yaml,linenums]
----
---
namespace: wordpress
secretGenerator:
- name: mysql-pass
  literals:
  - password=password123
resources:
  - mysql.yaml
  - wordpress.yaml
----

MySQL gets storage changed:

.https://j.mp/3cRFHSq[mysql.yml]
[source,yaml,linenums]
----
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  serviceName: mysql
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:8
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: wordpress-mysql
          mountPath: /var/lib/mysql
----

The change in the above is the addition of `storageClassName: openebs-jiva`. Now, our MySQL/MariaDB pod can move from node to node in our larger MicroK8s cluster.

Change the WordPress yaml file:

.https://j.mp/2MJJMNZ[wordpress.yml]
[source,yaml,linenums]
----
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wpk8s-club-demo
  labels:
    app: wpk8s-club-demo
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/from-to-www-redirect: "true"
spec:
  tls:
  - hosts:
    - demo.wpk8s.club
    - www.demo.wpk8s.club
    secretName: wpk8s-club-demo-tls
  rules:
  - host: demo.wpk8s.club
    http:
      paths:
        - pathType: Prefix
          path: "/"
          backend:
            service:
              name: wpk8s-club-demo
              port:
                number: 80

---
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: wordpress
    tier: frontend
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  serviceName: wordpress
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      initContainers:
      - name: init-mysql
        image: busybox
        command: ['sh', '-c', 'until nslookup mysql; do echo waiting for mysql; sleep 2; done;']
      containers:
      - image: wordpress:5.7
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress
          mountPath: /var/www/html
----

Let's ROCK: `sudo microk8s.kubectl apply -k ./`. Like before, will take a while, possibly up to 2 minutes on a fresh cluster that needs to pull container images, and our website will be available.

Now load the website.

=== PHP Sessions issue

So we have WordPress up and running, but we have a bug. Probably could be a feature, but depends on how you understand it.

WordPress, compared to some other php "frameworks", do not mess with PHP's session configuration and leave that in the hands of the person managing and configuring PHP. That is a good thing for who manages the hosting, but probably a pain for who needs special configuration of session.

There are 2 ways we can fix this. The old way for sticky sessions, which is a feature of nginx ingress controller, allowing us a logged in user to talk to same WordPress container, holding the same session, or the one I prefer, enhancing PHP to talk to a Memcached or Redis service and allow the requests to be balanced to all replicas for better spread of work load.

The first scenario allows you to stick to the official image easy, with no modifications, and I do recommend it for common WordPress websites. Although we could use a shared volume using OpenEBS NFS provisioner, handling sessions with shared files would decrease each response with dozens to hundreds of millisenconds and I find that not acceptable.

The second scenario, I always use it for WooCommerce websites, where I want to have a better Service Level Agreement. For this one, I deploy an enhanced image, that contains the extra php extensions to talk to Memcached or Redis. Also, I choosed to use one of this and not other central cache services, as they will work out of the box with caching plugins capable of using one of them. With this I manage to provide a high SLA and optimal performance.

Let's try scenario one, using Sticky Sessions.

.ingress.yml
[source,yaml,linenums]
----
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wpk8s-club-demo
  labels:
    app: wpk8s-club-demo
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/from-to-www-redirect: "true"
    nginx.ingress.kubernetes.io/affinity: "cookie"
    nginx.ingress.kubernetes.io/affinity-mode: "persistent"
    nginx.ingress.kubernetes.io/session-cookie-name: "INGRESSCOOKIE"
    nginx.ingress.kubernetes.io/session-cookie-samesite: "Lax"
spec:
  tls:
  - hosts:
    - demo.wpk8s.club
    - www.demo.wpk8s.club
    secretName: wpk8s-club-demo-tls
  rules:
  - host: demo.wpk8s.club
    http:
      paths:
        - pathType: Prefix
          path: "/"
          backend:
            service:
              name: wpk8s-club-demo
              port:
                number: 80
----

Run `kubectl apply -k .` and the problem will be gonne.

Let's go now with the more enhanced solution which should be prefered for ecommerce and mostly authenticated sessions websites.


