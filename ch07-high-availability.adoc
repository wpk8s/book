[[ch07-high-availability]]
== High availability

For me, high availability simply means that if part of the infrastructure
fails, the websites still run without problems for visitors, and the only
problem is for me to fix the infrastructure. Ideally, the infrastructure
should self-heal and auto-scale, a more advanced topicto be discussed later.

MicroK8s starts by default with HA enabled, and starts acting as a real
highly available cluster the moment you add 2 or more extra nodes in the
cluster, as the setup is 3 minimal nodes.

The missing part of default, which might not be needed for some cases, but
mandatory for a WordPress intended cluster, is Highly Available Storage.

=== Local storage vs highly available storage

MicroK8s comes with two alternative storage solution addons, at one step
distance to use any of it.

The one that existed since the beginning of the project, LOCAL, allowed
containers to use volumes strictly if they run on the same host.

That has been a blocker to high availability, because if that specific node
crashes, then data would not be readily available to continue running on
other healthy nodes in the cluster.

Since version 1.21, MicroK8s has https://openebs.io/[OpenEBS] as a storage
addon, and on any high availablity cluster with persistent data requirements
this must be your default choice, unless you already know how to handle
alternative solutions.

For alternative solutions, I would look into using the public clouds volume
attachment support in Kubernetes, but for me that would go against the concept of
being _public cloud vendor free_, although with some effort, it could be
done. But it would involve extra research, maintenance and so on. Also, it would
not allow horizontal scaling of WordPress, something that will be easy to do
in the near feature using OpenEBS, when
https://github.com/openebs/dynamic-nfs-provisioner[Dynamic NFS provisioner]
is stable and is added by default.

=== Introducing OpenEBS

For High Availablity storage, OpenEBS on MicroK8s will be the fastest with
 the benefit of minimal maintenance effort.

The addon comes with two storage classes, and I want you to remember one
thing: **openebs-jiva-default** is the only thing you need to use to
have self healing containers that can move from node to node without
loosing everything when a node crashes.

It does, however, come with a minimal resource requirement: a minimum of 3 nodes in the
cluster. Well, that is also the specification of MicroK8s High Availablity
cluster. The addon is pre-configured to replicate the volumes on a minimum of 3
nodes.

From this point, as we will be handling a High Availablity cluster, we will be
using a setup with a minimum of 3 nodes all the time.

NOTE: **Localpv storage class** is available as well. It will still allow any node
to mount a volume from this class, but, the volumes will be locked to the node in
which they were created. While there is a small performance benefit, in the case of a
disaster, that disaster would be the total unavailability of the affected websites.
So, I strongly recommend using Jiva, as node crashes will not be disasterous, just
temporary and quickly fixable issues.

**What is OpenEBS offering us?**

OpenEBS provides simply network compatible volumes, with real-time mirroring of
data, so in case of the crush of the node that is storing the data, at least
two other have the identical same data. If a container with a volume attached
to it crushes as of a node failure, it is restarted very quickly on another
node, and the volume is re-attached to it. Even if the node had a copy of the
volumes data, failure is not felt, as the volume can still run from the other
sources. With proper alerting system in place, you could jump in time and fix
the node's issue, or in my preference, I make sure I have the problem logs
available, and simply create a new node in the cluster, and remove the old
broken one. After getting my ideal state of the cluster back, I can now
research in the problem without having the cluster in danger, and the visitors
would not even know there was a problem.

Another advantage of OpenEBS is that one volume could be shared. Meaning we
will be able to use a volume attached to WordPress, spread on multiple nodes
to increase performance on high traffic. We can add nodes, we can remove nodes.
This helps keeping costs lower in time, without causing any downtime as you
need in classic vertical scaling setups.

One limitation at the writing of the book is that
https://github.com/openebs/dynamic-nfs-provisioner[Dynamic NFS Provisioner] was
not yet stable and included by default. I do hope the day OpenEBS marks it
stable, Canonical will update the included addon to support it with easy to
get it enabled option. This storage class would allow Read-Write-Many, so you
could pump the WordPress pod replicas to whatever number you need.

=== Setting up a Highly Available Cluster

Using the setup we did until now, we can replicate the setup from Chapter 4, at
least 3 times to have minimal 3 nodes available to setup the cluster. Just stop
right before `sudo microk8s enable dns ingress storage`.

The next step we need is to install and enable iscsi daemon, to allow OpenEBS
to work.

On each node, run the following:

[source,bash]
----
sudo apt install open-iscsi
sudo systemctl enable --now iscsid
----

Now, simply, only in one node, first one I usually do, run the command to get
the add node command with token:

`sudo microk8s add-node`

The result should be similar to:

[source,text]
----
From the node you wish to join to this cluster, run the following:
microk8s join 135.181.91.189:25000/f6308cb5dea38cb7c785adfe347a214a/eaf81c74de94

If the node you are adding is not reachable through the default interface you can use one of the following:
 microk8s join 135.181.91.189:25000/f6308cb5dea38cb7c785adfe347a214a/eaf81c74de94
 microk8s join 172.16.0.3:25000/f6308cb5dea38cb7c785adfe347a214a/eaf81c74de94
----

Copy the last line and run it prefixed with `sudo ` on second node.

Repeat the process for each node, first get command with token, second run it
on the new node.

You can have as many nodes you want like this.

Next, on first node, or another one if you would insist, run:

`sudo microk8s enable dns ingress openebs`

It will take some time, up to a minute.

When you are done, if you run `sudo microk8s status` you should have a similar
output:

[source,text]
----
microk8s is running
high-availability: yes
  datastore master nodes: 172.16.0.3:19001 172.16.0.5:19001 172.16.0.2:19001
  datastore standby nodes: 172.16.0.7:19001 172.16.0.6:19001 172.16.0.4:19001
addons:
  enabled:
    dns                  # CoreDNS
    ha-cluster           # Configure high availability on the current node
    helm3                # Helm 3 - Kubernetes package manager
    ingress              # Ingress controller for external access
    openebs              # OpenEBS is the open-source storage solution for Kubernetes
----

The rest of information is about disabled addons, so you can ignore the rest.

The above, shows I have a cluster of 6 nodes, with 3 masters and 3 worker
nodes that in case of a dead master, any could replace it. With more than 3
nodes I surely sleep well when 1-2 nodes crush during the night, as long
as the total amount of workload can be sustained by healthy nodes.

Regarding this, I usually calculate that I use 70-80% of amount of resources,
but I don't go under 3 nodes, if my low traffic load can use 1 node only.
We will talk about how to estimate resources later in the book.

Before we dive in WordPress, you need to redo the `cert-manager` addition, so
the cluster will be capable to handle automatically HTTPS needed certificates.
I will use the Letsencrypt option, as I prefer it. If you use alternatives,
adjust in the recipes to use the correct certificate.

Let's now tweak our original WordPress recipe. When we see it working, we will
experiment manually nodes issues, simulating common problems you might face
when for example, the public cloud provider has issues and certain nodes become
unavailable (we will simply destroy the node, which will be equivalent for our
cluster when node is not available because of the provider).

IMPORTANT: Database nodes can't be replicated by bumping up the number of
replicas. If you are looking into Mysql/MariaDB replication, than the only
easy way is https://mariadb.com/kb/en/galera-cluster/[Galera], possible by
defining multiple pods, as each server must be unique. Alternative, other
solutions exist, but with much more complex setup and administration.

I will edit the **WordPress** recipe we used until now, replacing the local
storage part with OpenEBS Jiva storage class.

NOTE: To ensure that I keep my recipes safe, I use git to keep changes. This
way, I can run them from any node, anytime, and keep changes synced. Personally
I use my first node usually like a "master" and do all operations only from it,
and in case it dies, I pick next one to be my "master", but the recipes, kept
in git, I just pull the latest and everything is still there. One note on it, I
keep secrets separate and add them by environment - I will show you later how I
keep secrets safe even in git. This recipes could be even be shared open, as do
not expose anything sensitive about your content and data.

Secrets stay like before:

.https://j.mp/3q0UdLp[kustomization.yml]
[source,yaml,linenums]
----
---
secretGenerator:
- name: mysql-pass
  literals:
  - password=password123
resources:
  - mysql-statefulset.yaml
  - wordpress-statefulset.yaml
----

MySQL gets storage changed:

.https://j.mp/3cRFHSq[mysql-statefulset.yml]
[source,yaml,linenums]
----
---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  serviceName: wordpress-mysql
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mariadb:10.5
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: wordpress-mysql
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: wordpress-mysql
    spec:
      storageClassName: openebs-jiva-default
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
----

The change in the above is in adding `storageClassName: openebs-jiva-default`.
Now, our MySQL/MariaDB pod can move from node to node in our larger MicroK8s
cluster.

Change the WordPress yaml file:

.https://j.mp/2MJJMNZ[wordpress-statefulset.yml]
[source,yaml,linenums]
----
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wpk8s-club-demo
  labels:
    app: wpk8s-club-demo
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/from-to-www-redirect: "true"
spec:
  tls:
  - hosts:
    - demo.wpk8s.club
    - www.demo.wpk8s.club
    secretName: wpk8s-club-demo-tls
  rules:
  - host: demo.wpk8s.club
    http:
      paths:
        - pathType: Prefix
          path: "/"
          backend:
            service:
              name: wpk8s-club-demo
              port:
                number: 80

---
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: wordpress
    tier: frontend
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  serviceName: wordpress
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      initContainers:
      - name: init-mysql
        image: busybox
        command: ['sh', '-c', 'until nslookup wordpress-mysql; do echo waiting for mysql; sleep 2; done;']
      containers:
      - image: wordpress:5.7
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: wordpress-mysql
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress
          mountPath: /var/www/html
  volumeClaimTemplates:
  - metadata:
      name: wordpress
    spec:
      storageClassName: openebs-jiva-default
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
----

Let's ROCK: `sudo microk8s.kubectl apply -k ./`. Like before, will take a few
seconds, possible up to 1-2 minutes on a fresh cluster that needs to pull
container images, and our website will be available.

Now load the website.
